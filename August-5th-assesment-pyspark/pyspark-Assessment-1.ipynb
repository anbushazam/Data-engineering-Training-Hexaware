{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#    Exercise 1.1 INTIALIZING SPARKSESSION"
      ],
      "metadata": {
        "id": "l0LHA69NzmUY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OSE_Z9yOqaSm"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder .appName(\"BotCampus Intermediate Session\") .master(\"local[*]\").getOrCreate()\n",
        "sc=spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Exercise 1.2-Load starter data"
      ],
      "metadata": {
        "id": "CYJHuC_pz4Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Ananya\", \"Bangalore\", 24),\n",
        "(\"Ravi\", \"Hyderabad\", 28),\n",
        "(\"Kavya\", \"Delhi\", 22),\n",
        "(\"Meena\", \"Chennai\", 25)]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgiFkIKW0AnX",
        "outputId": "85c68bdd-bb09-458f-a0b2-5025ab8220e5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Ananya|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Exercise 2.1 – Create RDD from feedback:"
      ],
      "metadata": {
        "id": "-LZZqF520Iz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = spark.sparkContext.parallelize([\n",
        "\"Ravi from Bangalore loved the mobile app\",\n",
        "\"Meena from Delhi reported poor response time\",\n",
        "\"Ajay from Pune liked the delivery speed\",\n",
        "\"Ananya from Hyderabad had an issue with UI\",\n",
        "\"Rohit from Mumbai gave positive feedback\"\n",
        "])"
      ],
      "metadata": {
        "id": "oDKbAxkG0QxP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#counting total number of words\n",
        "word_count=feedback.flatMap(lambda x:x.split())\n",
        "word_count.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gjvvmDV0aWS",
        "outputId": "7f42a969-22d9-4135-80a4-bb88cf2c6174"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#finding top 3 most common words\n",
        "word_count=feedback.flatMap(lambda x:x.split()).map(lambda x:(x.lower(),1)).reduceByKey(lambda x,y:x+y).takeOrdered(3,key=lambda pair: -pair[1])\n",
        "print(\"Top 3words:\",word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg8x8dE72KYK",
        "outputId": "76af3cdf-e835-4a4f-faed-55ff4c356da8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3words: [('from', 5), ('the', 2), ('loved', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words (from , with , the , etc.).\n",
        "stop_words = {\"from\", \"with\", \"the\", \"had\", \"gave\", \"an\", \"and\", \"to\", \"for\", \"on\", \"in\", \"a\"}\n",
        "filtered=feedback.flatMap(lambda x:x.split()).map(lambda x:x.lower()).filter(lambda x:x not in stop_words)\n",
        "filtered.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzBmzZmn3o4t",
        "outputId": "d5aa74f0-dc93-4a18-bd02-a073d342f958"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ravi',\n",
              " 'bangalore',\n",
              " 'loved',\n",
              " 'mobile',\n",
              " 'app',\n",
              " 'meena',\n",
              " 'delhi',\n",
              " 'reported',\n",
              " 'poor',\n",
              " 'response',\n",
              " 'time',\n",
              " 'ajay',\n",
              " 'pune',\n",
              " 'liked',\n",
              " 'delivery',\n",
              " 'speed',\n",
              " 'ananya',\n",
              " 'hyderabad',\n",
              " 'issue',\n",
              " 'ui',\n",
              " 'rohit',\n",
              " 'mumbai',\n",
              " 'positive',\n",
              " 'feedback']"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of word → count.\n",
        "word_dict=feedback.flatMap(lambda x:x.split()).map(lambda x:(x.lower(),1)).reduceByKey(lambda x,y:x+y).collectAsMap()\n",
        "print(word_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Lgwo3pG44wi",
        "outputId": "db56b76b-50c6-4fe9-9ee2-49ee7f2f3197"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'from': 5, 'loved': 1, 'app': 1, 'poor': 1, 'response': 1, 'liked': 1, 'speed': 1, 'ananya': 1, 'an': 1, 'issue': 1, 'with': 1, 'rohit': 1, 'mumbai': 1, 'positive': 1, 'feedback': 1, 'ravi': 1, 'bangalore': 1, 'the': 2, 'mobile': 1, 'meena': 1, 'delhi': 1, 'reported': 1, 'time': 1, 'ajay': 1, 'pune': 1, 'delivery': 1, 'hyderabad': 1, 'had': 1, 'ui': 1, 'gave': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. DataFrames – Transformations\n",
        " Exercise 3.1 – Create exam_scores\n",
        " DataFrame:"
      ],
      "metadata": {
        "id": "7nf7P_3y6NG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [\n",
        "(\"Ravi\", \"Math\", 88),\n",
        "(\"Ananya\", \"Science\", 92),\n",
        "(\"Kavya\", \"English\", 79),\n",
        "(\"Ravi\", \"English\", 67),\n",
        "(\"Neha\", \"Math\", 94),\n",
        "(\"Meena\", \"Science\", 85)\n",
        "]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_scores = spark.createDataFrame(scores, columns)\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueOceIQk6TZV",
        "outputId": "5eedc9e8-7f75-4309-9a76-6efec6f8f4f9"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+\n",
            "|  name|subject|score|\n",
            "+------+-------+-----+\n",
            "|  Ravi|   Math|   88|\n",
            "|Ananya|Science|   92|\n",
            "| Kavya|English|   79|\n",
            "|  Ravi|English|   67|\n",
            "|  Neha|   Math|   94|\n",
            "| Meena|Science|   85|\n",
            "+------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add grade column (>=90 → A, 80-89 → B, 70-79 → C, else D).\n",
        "from pyspark.sql.functions import col,when,sum\n",
        "df_scores=df_scores.withColumn(\"grade_column\",when(col('score')>=90,\"A\").when(col('score')>=80,\"B\").when(col('score')>=70,\"C\").otherwise(\"D\"))\n",
        "df_scores.show()\n",
        "print(\"---------------------------\")\n",
        " #Group by subject, find average score.\n",
        "grouped_sub=df_scores.groupBy(\"subject\").sum(\"score\")\n",
        "grouped_sub.show()\n",
        "print(\"------------------------------\")\n",
        "#Use when and otherwise to classify subject difficulty (Difficult).\n",
        "\n",
        "df_scores=df_scores.withColumn(\"subject difficulity\",when(col('subject').isin([\"Math\",\"Science\"]),\"Difficult\").otherwise(\"Easy\"))\n",
        "df_scores.show()\n",
        "print(\"------------------------------\")\n",
        "# Rank students per subject using Window function.\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "windowSpec=Window.partitionBy(\"subject\").orderBy(col(\"score\").desc())\n",
        "df_scores=df_scores.withColumn(\"rank\",rank().over(windowSpec))\n",
        "df_scores.show()\n",
        "print(\"------------------------------\")\n",
        "# Apply UDF to format names (e.g., make all uppercase)\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "to_upper=udf(lambda x:x.upper(),StringType())\n",
        "df_scores=df_scores.withColumn(\"name\",to_upper(col(\"name\")))\n",
        "df_scores.show()\n",
        "df_scores=df_scores.select(\"name\",\"subject\",\"score\",\"subject difficulity\",\"grade_column\",\"rank\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5son2ev_6brA",
        "outputId": "6c314985-7c8a-4018-f650-79e49404526d"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-------------------+------------+----+\n",
            "|  name|subject|score|subject difficulity|grade_column|rank|\n",
            "+------+-------+-----+-------------------+------------+----+\n",
            "| KAVYA|English|   79|               Easy|           C|   1|\n",
            "|  RAVI|English|   67|               Easy|           D|   2|\n",
            "|  NEHA|   Math|   94|          Difficult|           A|   1|\n",
            "|  RAVI|   Math|   88|          Difficult|           B|   2|\n",
            "|ANANYA|Science|   92|          Difficult|           A|   1|\n",
            "| MEENA|Science|   85|          Difficult|           B|   2|\n",
            "+------+-------+-----+-------------------+------------+----+\n",
            "\n",
            "---------------------------\n",
            "+-------+----------+\n",
            "|subject|sum(score)|\n",
            "+-------+----------+\n",
            "|Science|       177|\n",
            "|   Math|       182|\n",
            "|English|       146|\n",
            "+-------+----------+\n",
            "\n",
            "------------------------------\n",
            "+------+-------+-----+-------------------+------------+----+\n",
            "|  name|subject|score|subject difficulity|grade_column|rank|\n",
            "+------+-------+-----+-------------------+------------+----+\n",
            "| KAVYA|English|   79|               Easy|           C|   1|\n",
            "|  RAVI|English|   67|               Easy|           D|   2|\n",
            "|  NEHA|   Math|   94|          Difficult|           A|   1|\n",
            "|  RAVI|   Math|   88|          Difficult|           B|   2|\n",
            "|ANANYA|Science|   92|          Difficult|           A|   1|\n",
            "| MEENA|Science|   85|          Difficult|           B|   2|\n",
            "+------+-------+-----+-------------------+------------+----+\n",
            "\n",
            "------------------------------\n",
            "+------+-------+-----+-------------------+------------+----+\n",
            "|  name|subject|score|subject difficulity|grade_column|rank|\n",
            "+------+-------+-----+-------------------+------------+----+\n",
            "| KAVYA|English|   79|               Easy|           C|   1|\n",
            "|  RAVI|English|   67|               Easy|           D|   2|\n",
            "|  NEHA|   Math|   94|          Difficult|           A|   1|\n",
            "|  RAVI|   Math|   88|          Difficult|           B|   2|\n",
            "|ANANYA|Science|   92|          Difficult|           A|   1|\n",
            "| MEENA|Science|   85|          Difficult|           B|   2|\n",
            "+------+-------+-----+-------------------+------------+----+\n",
            "\n",
            "------------------------------\n",
            "+------+-------+-----+-------------------+------------+----+\n",
            "|  name|subject|score|subject difficulity|grade_column|rank|\n",
            "+------+-------+-----+-------------------+------------+----+\n",
            "| KAVYA|English|   79|               Easy|           C|   1|\n",
            "|  RAVI|English|   67|               Easy|           D|   2|\n",
            "|  NEHA|   Math|   94|          Difficult|           A|   1|\n",
            "|  RAVI|   Math|   88|          Difficult|           B|   2|\n",
            "|ANANYA|Science|   92|          Difficult|           A|   1|\n",
            "| MEENA|Science|   85|          Difficult|           B|   2|\n",
            "+------+-------+-----+-------------------+------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Ingest CSV & JSON – Save to Parquet\n",
        " Dataset 1: CSV file:\n",
        "students.csv"
      ],
      "metadata": {
        "id": "yOUdKe1gEfNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data=\"\"\"\n",
        "id,name,department,city,salary\n",
        "1,Amit,IT,Bangalore,78000\n",
        "2,Kavya,HR,Chennai,62000\n",
        "3,Arjun,Finance,Hyderabad,55000\"\"\"\n",
        "with open('students.csv','w')as f:\n",
        "  f.write(csv_data)"
      ],
      "metadata": {
        "id": "0HTwynx8EkWZ"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json_data=[\n",
        "    {\n",
        " \"id\": 101,\n",
        " \"name\": \"Sneha\",\n",
        " \"address\": {\n",
        "   \"city\": \"Mumbai\",\n",
        "   \"pincode\": 400001\n",
        "  },\n",
        " \"skills\": [\"Python\", \"Spark\"]\n",
        " }\n",
        "]\n",
        "with open('employee_nested.json','w')as f:\n",
        "  json.dump(json_data,f)"
      ],
      "metadata": {
        "id": "GOpDwV_XGCht"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading datasets into pyspark"
      ],
      "metadata": {
        "id": "HhgYTyaKIUdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sf=spark.read.csv('students.csv',header=True,inferSchema=True)\n",
        "\n",
        "ef=spark.read.json('employee_nested.json',multiLine=True)\n"
      ],
      "metadata": {
        "id": "ty-nIYzxITyS"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print schema and infer nested structure.\n",
        "sf.printSchema()\n",
        "ef.printSchema()\n",
        "sf.show()\n",
        "ef.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cpT3LZ_I9yO",
        "outputId": "ac14c2ec-5495-4bdf-fc89-a344b0f9d47d"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- pincode: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+---+-----+----------+---------+------+\n",
            "| id| name|department|     city|salary|\n",
            "+---+-----+----------+---------+------+\n",
            "|  1| Amit|        IT|Bangalore| 78000|\n",
            "|  2|Kavya|        HR|  Chennai| 62000|\n",
            "|  3|Arjun|   Finance|Hyderabad| 55000|\n",
            "+---+-----+----------+---------+------+\n",
            "\n",
            "+----------------+---+-----+---------------+\n",
            "|         address| id| name|         skills|\n",
            "+----------------+---+-----+---------------+\n",
            "|{Mumbai, 400001}|101|Sneha|[Python, Spark]|\n",
            "+----------------+---+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flattening json\n",
        "from pyspark.sql.functions import explode\n",
        "ef=ef.select(col(\"id\"),col(\"name\"),col(\"address.city\").alias(\"city\"),col(\"address.pincode\").alias(\"pincode\"),explode(col(\"skills\")).alias(\"skill\"))\n",
        "ef.show()\n",
        "#writing both files as parquet\n",
        "sf.write.mode(\"overwrite\").parquet(\"C:\\\\Users\\\\AnbuC\\\\OneDrive\\\\Desktop\\\\tmp\\\\students_parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXVcURKvKazP",
        "outputId": "f8e70146-c8c9-4ced-983c-b3d1e4c34e7d"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+-------+------+\n",
            "| id| name|  city|pincode| skill|\n",
            "+---+-----+------+-------+------+\n",
            "|101|Sneha|Mumbai| 400001|Python|\n",
            "|101|Sneha|Mumbai| 400001| Spark|\n",
            "+---+-----+------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#writing both files as parquet\n",
        "sf.write.mode(\"overwrite\").parquet(\"/tmp/output/students_parquet\")\n",
        "ef.write.mode(\"overwrite\").parquet(\"/tmp/output/employee_parquet\")\n"
      ],
      "metadata": {
        "id": "iXkP2N96NjEn"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Spark SQL – Temp Views & Queries\n",
        " Exercise 5.1 Create view from exam scores and run:"
      ],
      "metadata": {
        "id": "sVu-b2A6fcZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores.createOrReplaceTempView(\"exam_scores\")\n",
        "spark.sql(\"\"\"select subject,name,score,rn from (select*,rank() over (partition by subject order by score desc)  as rn from exam_scores as tmp)\n",
        " where rn=1 \"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iABRyvkfgKM",
        "outputId": "b5d23220-e2dc-40bf-a7d6-6914dbd7068b"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----+---+\n",
            "|subject|  name|score| rn|\n",
            "+-------+------+-----+---+\n",
            "|English| KAVYA|   79|  1|\n",
            "|   Math|  NEHA|   94|  1|\n",
            "|Science|ANANYA|   92|  1|\n",
            "+-------+------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count of students per grade\n",
        "spark.sql(\"\"\"select grade_column,count(*)as count_students from exam_scores group by grade_column\"\"\").show()\n",
        "\n",
        "#students with multiple subjects\n",
        "\n",
        "spark.sql(\"\"\"select name from (select name,count(subject)as count_multiple_subjects from exam_scores group by name)as tmp where count_multiple_subjects >1\"\"\").show()\n",
        "\n",
        "# subjects with average score above 85\n",
        "spark.sql(\"\"\"select subject,avg(score) as avg_score from exam_scores group by subject having avg_score >85 \"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nklCP-bRiZO5",
        "outputId": "aea609d5-185b-4347-8713-8390ffb3e275"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------+\n",
            "|grade_column|count_students|\n",
            "+------------+--------------+\n",
            "|           B|             2|\n",
            "|           C|             1|\n",
            "|           A|             2|\n",
            "|           D|             1|\n",
            "+------------+--------------+\n",
            "\n",
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|RAVI|\n",
            "+----+\n",
            "\n",
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|Science|     88.5|\n",
            "|   Math|     91.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Exercise 5.2 Create another DataFrame\n",
        "attendance(name, days_present)"
      ],
      "metadata": {
        "id": "N8h9Q0hbmzGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "att_data = [(\"RAVI\", 22), (\"ANANYA\", 18), (\"KAVYA\", 25), (\"MEENA\", 19), (\"NEHA\", 20)]\n",
        "df_att = spark.createDataFrame(att_data, [\"name\", \"days_present\"])\n",
        "df_att.show()\n",
        "print(\"-----------------\")\n",
        "\n",
        "# joining with score\n",
        "joined_df=df_scores.join(df_att,on='name',how='left')\n",
        "joined_df.show()\n",
        "#Calculate attendance-adjusted grade:\n",
        "print(\"---------------\")\n",
        "from pyspark.sql.functions import expr\n",
        "grade_order=expr(\"\"\"\n",
        "              case\n",
        "               when days_present < 20 and grade_column=\"A\" then \"B\"\n",
        "               when days_present < 20 and grade_column=\"B\" then \"c\"\n",
        "               when days_present < 20 and grade_column=\"c\" then \"D\"\n",
        "               else grade_column\n",
        "               end\n",
        "\n",
        " \"\"\")\n",
        "joined_df.withColumn(\"grade_adjusted\",grade_order).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj3wHLELm7if",
        "outputId": "7f219526-5776-4839-93f2-8008c909f17d"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+\n",
            "|  name|days_present|\n",
            "+------+------------+\n",
            "|  RAVI|          22|\n",
            "|ANANYA|          18|\n",
            "| KAVYA|          25|\n",
            "| MEENA|          19|\n",
            "|  NEHA|          20|\n",
            "+------+------------+\n",
            "\n",
            "-----------------\n",
            "+------+-------+-----+-------------------+------------+----+------------+\n",
            "|  name|subject|score|subject difficulity|grade_column|rank|days_present|\n",
            "+------+-------+-----+-------------------+------------+----+------------+\n",
            "|  NEHA|   Math|   94|          Difficult|           A|   1|          20|\n",
            "| MEENA|Science|   85|          Difficult|           B|   2|          19|\n",
            "|  RAVI|English|   67|               Easy|           D|   2|          22|\n",
            "|  RAVI|   Math|   88|          Difficult|           B|   2|          22|\n",
            "|ANANYA|Science|   92|          Difficult|           A|   1|          18|\n",
            "| KAVYA|English|   79|               Easy|           C|   1|          25|\n",
            "+------+-------+-----+-------------------+------------+----+------------+\n",
            "\n",
            "---------------\n",
            "+------+-------+-----+-------------------+------------+----+------------+--------------+\n",
            "|  name|subject|score|subject difficulity|grade_column|rank|days_present|grade_adjusted|\n",
            "+------+-------+-----+-------------------+------------+----+------------+--------------+\n",
            "|  NEHA|   Math|   94|          Difficult|           A|   1|          20|             A|\n",
            "| MEENA|Science|   85|          Difficult|           B|   2|          19|             c|\n",
            "|  RAVI|English|   67|               Easy|           D|   2|          22|             D|\n",
            "|  RAVI|   Math|   88|          Difficult|           B|   2|          22|             B|\n",
            "|ANANYA|Science|   92|          Difficult|           A|   1|          18|             B|\n",
            "| KAVYA|English|   79|               Easy|           C|   1|          25|             C|\n",
            "+------+-------+-----+-------------------+------------+----+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Partitioned Load (Full + Incremental)"
      ],
      "metadata": {
        "id": "mg8OKh4Uq7e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Load:\n",
        "df_scores.write.partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "id": "Vq5IhUNkq-sD"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# incremental load\n",
        "incremental = [(\"Meena\", \"Math\", 93)]\n",
        "df_inc = spark.createDataFrame(incremental, columns)\n",
        "df_inc.write.mode(\"append\").partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "id": "WSv8TDarrWvQ"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#listing all folder inside  /tmp/scres\n",
        "import os\n",
        "print(\"partitions:\"),os.listdir(\"/tmp/scores/\")\n",
        "#reading only math partion\n",
        "df_math = spark.read.parquet(\"/tmp/scores/subject=Math\")\n",
        "df_math.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gplUsJAPrdGZ",
        "outputId": "bdc19901-9ad0-47d2-fe71-ad3d5398aafe"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "partitions:\n",
            "+-----+-----+-------------------+------------+----+\n",
            "| name|score|subject difficulity|grade_column|rank|\n",
            "+-----+-----+-------------------+------------+----+\n",
            "| NEHA|   94|          Difficult|           A|   1|\n",
            "| RAVI|   88|          Difficult|           B|   2|\n",
            "|Meena|   93|               NULL|        NULL|NULL|\n",
            "+-----+-----+-------------------+------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ETL: Clean, Transform, Load"
      ],
      "metadata": {
        "id": "UnJAZTF-sQkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = [\n",
        "    (1, \"Arjun\", \"IT\",      78000, 5000),\n",
        "    (2, \"Kavya\", \"HR\",      62000, None),\n",
        "    (3, \"Sneha\", \"Finance\", 55000, 3000)\n",
        "]\n",
        "cols = [\"emp_id\", \"name\", \"dept\", \"salary\", \"bonus\"]\n",
        "\n",
        "from pyspark.sql.functions import coalesce, lit\n",
        "\n",
        "df_raw = (\n",
        "    spark.createDataFrame(raw_data, cols).withColumn(\"bonus\", coalesce(col(\"bonus\"), lit(2000)))\n",
        ")\n",
        "df_raw.show()\n",
        "print(\"---------------\")\n",
        "# Calculate  total_ctc\n",
        "df_raw=df_raw.withColumn(\"total_ctc\",col('salary')+col('bonus'))\n",
        "df_raw.show()\n",
        "print(\"----------------\")\n",
        "\n",
        "#filter total_ctc>60000\n",
        "df_filter=df_raw.filter(col('total_ctc')>60000)\n",
        "df_filter.show()\n",
        "print(\"--------------\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sua-O0KRuwBW",
        "outputId": "625da717-f59d-4d4f-a83d-779ea1f8f944"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+\n",
            "|emp_id| name|   dept|salary|bonus|\n",
            "+------+-----+-------+------+-----+\n",
            "|     1|Arjun|     IT| 78000| 5000|\n",
            "|     2|Kavya|     HR| 62000| 2000|\n",
            "|     3|Sneha|Finance| 55000| 3000|\n",
            "+------+-----+-------+------+-----+\n",
            "\n",
            "---------------\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|emp_id| name|   dept|salary|bonus|total_ctc|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|     1|Arjun|     IT| 78000| 5000|    83000|\n",
            "|     2|Kavya|     HR| 62000| 2000|    64000|\n",
            "|     3|Sneha|Finance| 55000| 3000|    58000|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "\n",
            "----------------\n",
            "+------+-----+----+------+-----+---------+\n",
            "|emp_id| name|dept|salary|bonus|total_ctc|\n",
            "+------+-----+----+------+-----+---------+\n",
            "|     1|Arjun|  IT| 78000| 5000|    83000|\n",
            "|     2|Kavya|  HR| 62000| 2000|    64000|\n",
            "+------+-----+----+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving final dataframe to parquet and json\n",
        "df_filter.write.mode(\"overwrite\").parquet(\"/tmp/output/final_ctc.parquet\")\n",
        "df_filter.write.mode(\"overwrite\").json(\"/tmp/output/final_ctc.json\")"
      ],
      "metadata": {
        "id": "8iH-JZXYvysz"
      },
      "execution_count": 210,
      "outputs": []
    }
  ]
}