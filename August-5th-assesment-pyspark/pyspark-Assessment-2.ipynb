{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Module 1: Setup & SparkSession Initialization"
      ],
      "metadata": {
        "id": "1VTiWvJH7e1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MNxMNGhY3iAG"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder .appName(\"BotCampus PySpark Practice\") .master(\"local[*]\") .getOrCreate()\n",
        "sc=spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from:\n",
        "data = [\n",
        "(\"Anjali\", \"Bangalore\", 24),\n",
        "(\"Ravi\", \"Hyderabad\", 28),\n",
        "(\"Kavya\", \"Delhi\", 22),\n",
        "(\"Meena\", \"Chennai\", 25),\n",
        "(\"Arjun\", \"Mumbai\", 30)\n",
        "]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "df=spark.createDataFrame(data,columns)\n",
        "df.show()\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdg6VMIW7w5f",
        "outputId": "fa42ef8e-51fe-4f84-9db3-2b89f301595e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Anjali|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "| Arjun|   Mumbai| 30|\n",
            "+------+---------+---+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=df.rdd\n",
        "collected=rdd.collect()\n",
        "mapped=rdd.map(lambda x:(x.name,x.age+1)).collect()\n",
        "print(collected)\n",
        "print(mapped)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBSwrXJt8mvs",
        "outputId": "3d53a5f8-5d37-4f5d-d57e-e5779b90bb06"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(name='Anjali', city='Bangalore', age=24), Row(name='Ravi', city='Hyderabad', age=28), Row(name='Kavya', city='Delhi', age=22), Row(name='Meena', city='Chennai', age=25), Row(name='Arjun', city='Mumbai', age=30)]\n",
            "[('Anjali', 25), ('Ravi', 29), ('Kavya', 23), ('Meena', 26), ('Arjun', 31)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2: RDDs & Transformations"
      ],
      "metadata": {
        "id": "m2nracZa9vg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = spark.sparkContext.parallelize([\n",
        "\"Ravi from Bangalore loved the delivery\",\n",
        "\"Meena from Hyderabad had a late order\",\n",
        "\"Ajay from Pune liked the service\",\n",
        "\"Anjali from Delhi faced UI issues\",\n",
        "\"Rohit from Mumbai gave positive feedback\"\n",
        "])\n"
      ],
      "metadata": {
        "id": "jSxGmZPt9ynl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting words from feedback\n",
        "splitted_words=feedback.flatMap(lambda x:x.split())\n",
        "splitted_words.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gaMy9zi98sg",
        "outputId": "f77d3a61-15ac-4817-9ef3-c7436703b30d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ravi',\n",
              " 'from',\n",
              " 'Bangalore',\n",
              " 'loved',\n",
              " 'the',\n",
              " 'delivery',\n",
              " 'Meena',\n",
              " 'from',\n",
              " 'Hyderabad',\n",
              " 'had',\n",
              " 'a',\n",
              " 'late',\n",
              " 'order',\n",
              " 'Ajay',\n",
              " 'from',\n",
              " 'Pune',\n",
              " 'liked',\n",
              " 'the',\n",
              " 'service',\n",
              " 'Anjali',\n",
              " 'from',\n",
              " 'Delhi',\n",
              " 'faced',\n",
              " 'UI',\n",
              " 'issues',\n",
              " 'Rohit',\n",
              " 'from',\n",
              " 'Mumbai',\n",
              " 'gave',\n",
              " 'positive',\n",
              " 'feedback']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing stop words\n",
        "stop_words = {\"from\", \"the\", \"had\", \"a\", \"and\", \"of\", \"to\", \"in\"}\n",
        "\n",
        "remove_stop=splitted_words.filter(lambda x:x not in stop_words)\n",
        "remove_stop.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrfX710U-00D",
        "outputId": "ff3f36eb-d0ad-4a21-91cc-fe9fa238bf36"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ravi',\n",
              " 'Bangalore',\n",
              " 'loved',\n",
              " 'delivery',\n",
              " 'Meena',\n",
              " 'Hyderabad',\n",
              " 'late',\n",
              " 'order',\n",
              " 'Ajay',\n",
              " 'Pune',\n",
              " 'liked',\n",
              " 'service',\n",
              " 'Anjali',\n",
              " 'Delhi',\n",
              " 'faced',\n",
              " 'UI',\n",
              " 'issues',\n",
              " 'Rohit',\n",
              " 'Mumbai',\n",
              " 'gave',\n",
              " 'positive',\n",
              " 'feedback']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count each word frequency using reduceKey\n",
        "word_count=splitted_words.map(lambda x:(x.lower(),1)).reduceByKey(lambda x,y:x+y)\n",
        "word_count.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1p7Pz8r-6HS",
        "outputId": "56a4250c-c0c7-4ea0-e88e-1af8d15256f6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('from', 5),\n",
              " ('loved', 1),\n",
              " ('liked', 1),\n",
              " ('service', 1),\n",
              " ('anjali', 1),\n",
              " ('faced', 1),\n",
              " ('issues', 1),\n",
              " ('rohit', 1),\n",
              " ('mumbai', 1),\n",
              " ('positive', 1),\n",
              " ('feedback', 1),\n",
              " ('ravi', 1),\n",
              " ('bangalore', 1),\n",
              " ('the', 2),\n",
              " ('delivery', 1),\n",
              " ('meena', 1),\n",
              " ('hyderabad', 1),\n",
              " ('had', 1),\n",
              " ('a', 1),\n",
              " ('late', 1),\n",
              " ('order', 1),\n",
              " ('ajay', 1),\n",
              " ('pune', 1),\n",
              " ('delhi', 1),\n",
              " ('ui', 1),\n",
              " ('gave', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#top 3 non stop words\n",
        "non_stop=word_count.filter(lambda x:x[0] not in stop_words)\n",
        "non_stop=non_stop.sortBy(lambda x:x[1],ascending=False).take(3)\n",
        "print(non_stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hjRQlYH_qay",
        "outputId": "c71fd13b-42ce-4dd1-9627-f4ff8bb0af16"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('loved', 1), ('liked', 1), ('service', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3: DataFrames & Transformation (With Joins)"
      ],
      "metadata": {
        "id": "lBKYBNxeAsR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "students = [\n",
        "(\"Amit\", \"10-A\", 89),\n",
        "(\"Kavya\", \"10-B\", 92),\n",
        "(\"Anjali\", \"10-A\", 78),\n",
        "(\"Rohit\", \"10-B\", 85),\n",
        "(\"Sneha\", \"10-C\", 80)\n",
        "]\n",
        "columns = [\"name\", \"section\", \"marks\"]\n",
        "attendance = [\n",
        "(\"Amit\", 24),\n",
        "(\"Kavya\", 22),\n",
        "(\"Anjali\", 20),\n",
        "(\"Rohit\", 25),\n",
        "(\"Sneha\", 19)\n",
        "]\n",
        "columns2 = [\"name\", \"days_present\"]\n"
      ],
      "metadata": {
        "id": "XGlxk4XGApRn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sf=spark.createDataFrame(students,columns)\n",
        "sf.show()\n",
        "af=spark.createDataFrame(attendance,columns2)\n",
        "af.show()\n",
        "# Join both DataFrames on name\n",
        "print(\"----------------\")\n",
        "joined_df=sf.join(af,on=\"name\",how=\"left\")\n",
        "joined_df.show()\n",
        "print(\"--------------\")\n",
        "#Create a new column: attendance_rate = days_present / 25 .\n",
        "from pyspark.sql.functions import col,when\n",
        "joined_df=joined_df.withColumn(\"attendence_rate\",col('days_present')/25)\n",
        "joined_df.show()\n",
        "#grade students using when\n",
        "print(\"------------------\")\n",
        "joined_df=joined_df.withColumn(\"grades\",when(col('marks')>90,\"A\").when(col('marks')>=80,\"B\").otherwise(\"C\"))\n",
        "joined_df.show()\n",
        "print(\"------------------------\")\n",
        "# filter students with good grades but poor attendence\n",
        "filtered=joined_df.filter((col('grades').isin('A','B'))&(col('attendence_rate')<0.80))\n",
        "filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1zmSPDaBC58",
        "outputId": "e49b4caf-699f-41f4-9c1e-7d180288311a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+\n",
            "|  name|section|marks|\n",
            "+------+-------+-----+\n",
            "|  Amit|   10-A|   89|\n",
            "| Kavya|   10-B|   92|\n",
            "|Anjali|   10-A|   78|\n",
            "| Rohit|   10-B|   85|\n",
            "| Sneha|   10-C|   80|\n",
            "+------+-------+-----+\n",
            "\n",
            "+------+------------+\n",
            "|  name|days_present|\n",
            "+------+------------+\n",
            "|  Amit|          24|\n",
            "| Kavya|          22|\n",
            "|Anjali|          20|\n",
            "| Rohit|          25|\n",
            "| Sneha|          19|\n",
            "+------+------------+\n",
            "\n",
            "----------------\n",
            "+------+-------+-----+------------+\n",
            "|  name|section|marks|days_present|\n",
            "+------+-------+-----+------------+\n",
            "|  Amit|   10-A|   89|          24|\n",
            "| Kavya|   10-B|   92|          22|\n",
            "| Sneha|   10-C|   80|          19|\n",
            "| Rohit|   10-B|   85|          25|\n",
            "|Anjali|   10-A|   78|          20|\n",
            "+------+-------+-----+------------+\n",
            "\n",
            "--------------\n",
            "+------+-------+-----+------------+---------------+\n",
            "|  name|section|marks|days_present|attendence_rate|\n",
            "+------+-------+-----+------------+---------------+\n",
            "|  Amit|   10-A|   89|          24|           0.96|\n",
            "| Kavya|   10-B|   92|          22|           0.88|\n",
            "| Sneha|   10-C|   80|          19|           0.76|\n",
            "| Rohit|   10-B|   85|          25|            1.0|\n",
            "|Anjali|   10-A|   78|          20|            0.8|\n",
            "+------+-------+-----+------------+---------------+\n",
            "\n",
            "------------------\n",
            "+------+-------+-----+------------+---------------+------+\n",
            "|  name|section|marks|days_present|attendence_rate|grades|\n",
            "+------+-------+-----+------------+---------------+------+\n",
            "|  Amit|   10-A|   89|          24|           0.96|     B|\n",
            "| Kavya|   10-B|   92|          22|           0.88|     A|\n",
            "| Sneha|   10-C|   80|          19|           0.76|     B|\n",
            "| Rohit|   10-B|   85|          25|            1.0|     B|\n",
            "|Anjali|   10-A|   78|          20|            0.8|     C|\n",
            "+------+-------+-----+------------+---------------+------+\n",
            "\n",
            "------------------------\n",
            "+-----+-------+-----+------------+---------------+------+\n",
            "| name|section|marks|days_present|attendence_rate|grades|\n",
            "+-----+-------+-----+------------+---------------+------+\n",
            "|Sneha|   10-C|   80|          19|           0.76|     B|\n",
            "+-----+-------+-----+------------+---------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4: Ingest CSV & JSON, Save to Parquet"
      ],
      "metadata": {
        "id": "WYTNTD-aDzmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data=\"\"\"\n",
        "emp_id,name,dept,city,salary\n",
        "101,Anil,IT,Bangalore,80000\n",
        "102,Kiran,HR,Mumbai,65000\n",
        "103,Deepa,Finance,Chennai,72000\"\"\"\n",
        "with open(\"employee.csv\",'w')as f:\n",
        "  f.write(csv_data)\n",
        "import json\n",
        "json_data={\n",
        "    \"id\": 201,\n",
        "    \"name\": \"Nandini\",\n",
        "    \"contact\": {\n",
        "     \"email\": \"nandi@example.com\",\n",
        "     \"city\": \"Hyderabad\"\n",
        "     },\n",
        "    \"skills\": [\"Python\", \"Spark\", \"SQL\"]\n",
        "}\n",
        "with open('employee.json','w')as ff:\n",
        "  json.dump(json_data,ff)"
      ],
      "metadata": {
        "id": "hnqa-sHKD2z-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading both into dataframes\n",
        "csv_df=spark.read.csv(\"employee.csv\",header=True,inferSchema=True)\n",
        "json_df=spark.read.json(\"employee.json\")\n",
        "csv_df.show()\n",
        "json_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUN3oGZRE8Cb",
        "outputId": "e01c9cc9-0d57-4c04-d5e6-752406d78c24"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+---------+------+\n",
            "|emp_id| name|   dept|     city|salary|\n",
            "+------+-----+-------+---------+------+\n",
            "|   101| Anil|     IT|Bangalore| 80000|\n",
            "|   102|Kiran|     HR|   Mumbai| 65000|\n",
            "|   103|Deepa|Finance|  Chennai| 72000|\n",
            "+------+-----+-------+---------+------+\n",
            "\n",
            "+--------------------+---+-------+--------------------+\n",
            "|             contact| id|   name|              skills|\n",
            "+--------------------+---+-------+--------------------+\n",
            "|{Hyderabad, nandi...|201|Nandini|[Python, Spark, SQL]|\n",
            "+--------------------+---+-------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flattening json nested\n",
        "from pyspark.sql.functions import explode\n",
        "flat_json=json_df.select(col(\"id\"),col(\"name\"),col('contact.city').alias(\"city\"),col('contact.email').alias(\"email\"),explode(col(\"skills\")).alias(\"skill\"))\n",
        "flat_json.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQcrGClHFKmn",
        "outputId": "ca0e1b1b-7e2a-4e67-cc9e-7e7d95ae5f5a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---------+-----------------+------+\n",
            "| id|   name|     city|            email| skill|\n",
            "+---+-------+---------+-----------------+------+\n",
            "|201|Nandini|Hyderabad|nandi@example.com|Python|\n",
            "|201|Nandini|Hyderabad|nandi@example.com| Spark|\n",
            "|201|Nandini|Hyderabad|nandi@example.com|   SQL|\n",
            "+---+-------+---------+-----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the files partition in city\n",
        "csv_df.write.mode('overwrite').partitionBy('city').parquet(\"output/csv_parquet/\")\n",
        "flat_json.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"output/csv_parquet/\")"
      ],
      "metadata": {
        "id": "aS5nHXc3GPvf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 5: Spark SQL with Temp Views"
      ],
      "metadata": {
        "id": "vMaVL0oEGx4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "joined_df.createOrReplaceTempView(\"students_view\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    select section, avg(marks) as avg_marks\n",
        "    from students_view\n",
        "    group by section\n",
        "\"\"\").show()\n",
        "\n",
        "# 2. top scorer in each section\n",
        "spark.sql(\"\"\"\n",
        "    select section, name, marks\n",
        "    from (\n",
        "      select *,\n",
        "             row_number() over (\n",
        "               partition by section\n",
        "               order by marks desc\n",
        "             ) as rn\n",
        "      from students_view\n",
        "    ) tmp\n",
        "    where rn = 1\n",
        "\"\"\").show()\n",
        "\n",
        "# 3. count of students in each grade category\n",
        "spark.sql(\"\"\"\n",
        "    select grades, count(*) as count\n",
        "    from students_view\n",
        "    group by grades\n",
        "\"\"\").show()\n",
        "\n",
        "# 4. students with marks above class average\n",
        "spark.sql(\"\"\"\n",
        "    with avg_tbl as (\n",
        "      select avg(marks) as class_avg\n",
        "      from students_view\n",
        "    )\n",
        "    select s.name, s.marks\n",
        "    from students_view s\n",
        "    cross join avg_tbl\n",
        "    where s.marks > avg_tbl.class_avg\n",
        "\"\"\").show()\n",
        "\n",
        "# 5. attendance-adjusted performance\n",
        "spark.sql(\"\"\"\n",
        "    select\n",
        "      name,\n",
        "      marks * attendence_rate as adjusted_score\n",
        "    from students_view\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7kVn5TmHHpY",
        "outputId": "383d0173-e648-40ea-9ab2-14c92e6a570a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|section|avg_marks|\n",
            "+-------+---------+\n",
            "|   10-C|     80.0|\n",
            "|   10-A|     83.5|\n",
            "|   10-B|     88.5|\n",
            "+-------+---------+\n",
            "\n",
            "+-------+-----+-----+\n",
            "|section| name|marks|\n",
            "+-------+-----+-----+\n",
            "|   10-A| Amit|   89|\n",
            "|   10-B|Kavya|   92|\n",
            "|   10-C|Sneha|   80|\n",
            "+-------+-----+-----+\n",
            "\n",
            "+------+-----+\n",
            "|grades|count|\n",
            "+------+-----+\n",
            "|     B|    3|\n",
            "|     C|    1|\n",
            "|     A|    1|\n",
            "+------+-----+\n",
            "\n",
            "+-----+-----+\n",
            "| name|marks|\n",
            "+-----+-----+\n",
            "| Amit|   89|\n",
            "|Kavya|   92|\n",
            "|Rohit|   85|\n",
            "+-----+-----+\n",
            "\n",
            "+------+------------------+\n",
            "|  name|    adjusted_score|\n",
            "+------+------------------+\n",
            "|  Amit|             85.44|\n",
            "| Kavya|             80.96|\n",
            "| Sneha|              60.8|\n",
            "| Rohit|              85.0|\n",
            "|Anjali|62.400000000000006|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 6: Partitioned Data & Incremental Loading"
      ],
      "metadata": {
        "id": "uDHFWlaNIwgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Full Load\n",
        "sf.write.partitionBy(\"section\").parquet(\"output/students/\")"
      ],
      "metadata": {
        "id": "wOHrr2-SI0BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 2 incremental load\n",
        "incremental = [(\"Tejas\", \"10-A\", 91)]\n",
        "df_inc = spark.createDataFrame(incremental, [\"name\", \"section\", \"marks\"])\n",
        "df_inc.write.mode(\"append\").partitionBy(\"section\").parquet(\"output/students/\")\n"
      ],
      "metadata": {
        "id": "9tHFjsU6JHBJ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #List files in output/students/ using Python.\n",
        "import os\n",
        "for file in os.listdir(\"output/students/\"):\n",
        "  print(file)\n",
        "# Read only partition 10-A and list students.\n",
        "sec_students=spark.read.parquet(\"output/students/section=10-A\")\n",
        "sec_students.show()\n",
        " #Compare before/after counts for section 10-A\n",
        "df_10A = spark.read.parquet(\"output/students/section=10-A/\")\n",
        "print(\"Before/After Counts for 10-A:\", df_10A.count())\n",
        "df_10A.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah6QvUTlJXt8",
        "outputId": "673d3851-b396-4c16-91c6-368a03c3dc28"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "._SUCCESS.crc\n",
            "section=10-A\n",
            "section=10-B\n",
            "_SUCCESS\n",
            "section=10-C\n",
            "+------+-----+\n",
            "|  name|marks|\n",
            "+------+-----+\n",
            "|Anjali|   78|\n",
            "| Tejas|   91|\n",
            "|  Amit|   89|\n",
            "+------+-----+\n",
            "\n",
            "Before/After Counts for 10-A: 3\n",
            "+------+-----+\n",
            "|  name|marks|\n",
            "+------+-----+\n",
            "|Anjali|   78|\n",
            "| Tejas|   91|\n",
            "|  Amit|   89|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 7: ETL Pipeline â€“ End to End\n"
      ],
      "metadata": {
        "id": "KOSZg8hkLAI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data=\"\"\"\n",
        "emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,75000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,68000,4000\n",
        "4,Ramesh,Sales,58000,\"\"\"\n",
        "with open(\"employee_raw.csv\",'w')as f:\n",
        "  f.write(raw_data)\n"
      ],
      "metadata": {
        "id": "aSyb9U9kLEM4"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_employee=spark.read.csv(\"employee_raw.csv\",header=True,inferSchema=True)\n",
        "raw_employee.show()\n",
        "#filling bonus null with 2000\n",
        "raw_employee=raw_employee.fillna(value=2000,subset=['bonus'])\n",
        "raw_employee.show()\n",
        "#creating total_ctc\n",
        "from pyspark.sql.functions import col\n",
        "raw_employee=raw_employee.withColumn(\"total_ctc\",col('salary')+col('bonus'))\n",
        "raw_employee.show()\n",
        "#filter employees 65000 above ctc\n",
        "filtered_employee_ctc=raw_employee.filter(col('total_ctc')>65000)\n",
        "filtered_employee_ctc.show()\n",
        "#saving result in json format\n",
        "filtered_employee_ctc.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .json(\"/content/filtered_ctc.json\")\n",
        "\n",
        "# write partitioned parquet (overwrite)\n",
        "filtered_employee_ctc.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"dept\") \\\n",
        "    .parquet(\"/content/filtered_ctc_parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g1dLyZvLT-2",
        "outputId": "9cea5982-6272-456c-92d0-88969deba1ce"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+-----+\n",
            "|emp_id|  name|   dept|salary|bonus|\n",
            "+------+------+-------+------+-----+\n",
            "|     1| Arjun|     IT| 75000| 5000|\n",
            "|     2| Kavya|     HR| 62000| NULL|\n",
            "|     3| Sneha|Finance| 68000| 4000|\n",
            "|     4|Ramesh|  Sales| 58000| NULL|\n",
            "+------+------+-------+------+-----+\n",
            "\n",
            "+------+------+-------+------+-----+\n",
            "|emp_id|  name|   dept|salary|bonus|\n",
            "+------+------+-------+------+-----+\n",
            "|     1| Arjun|     IT| 75000| 5000|\n",
            "|     2| Kavya|     HR| 62000| 2000|\n",
            "|     3| Sneha|Finance| 68000| 4000|\n",
            "|     4|Ramesh|  Sales| 58000| 2000|\n",
            "+------+------+-------+------+-----+\n",
            "\n",
            "+------+------+-------+------+-----+---------+\n",
            "|emp_id|  name|   dept|salary|bonus|total_ctc|\n",
            "+------+------+-------+------+-----+---------+\n",
            "|     1| Arjun|     IT| 75000| 5000|    80000|\n",
            "|     2| Kavya|     HR| 62000| 2000|    64000|\n",
            "|     3| Sneha|Finance| 68000| 4000|    72000|\n",
            "|     4|Ramesh|  Sales| 58000| 2000|    60000|\n",
            "+------+------+-------+------+-----+---------+\n",
            "\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|emp_id| name|   dept|salary|bonus|total_ctc|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|     1|Arjun|     IT| 75000| 5000|    80000|\n",
            "|     3|Sneha|Finance| 68000| 4000|    72000|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "evQomEnKOf3X"
      },
      "execution_count": 74,
      "outputs": []
    }
  ]
}