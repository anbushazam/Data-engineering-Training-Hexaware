{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMnCa6Ss3Vth",
        "outputId": "81738c06-35bb-4c64-d53f-3f83a7e8b373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID|Name |Department |Project        |Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|101  |Ravi |Engineering|AI Engine      |95000 |42          |\n",
            "|102  |Sneha|Engineering|Data Platform  |87000 |45          |\n",
            "|103  |Kabir|Marketing  |Product Launch |65000 |40          |\n",
            "|104  |Anita|Sales      |Client Outreach|70000 |38          |\n",
            "|105  |Divya|Engineering|AI Engine      |99000 |48          |\n",
            "|106  |Amit |Marketing  |Social Media   |62000 |35          |\n",
            "|107  |Priya|HR         |Policy Revamp  |58000 |37          |\n",
            "|108  |Manav|Sales      |Lead Gen       |73000 |41          |\n",
            "|109  |Neha |Engineering|Security Suite |91000 |46          |\n",
            "|110  |Farah|HR         |Onboarding     |60000 |36          |\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark sql exercises\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample employee data\n",
        "data = [\n",
        "Row(EmpID=101, Name=\"Ravi\", Department=\"Engineering\", Project=\"AI Engine\", Salary=95000, HoursPerWeek=42),\n",
        "Row(EmpID=102, Name=\"Sneha\", Department=\"Engineering\", Project=\"Data Platform\", Salary=87000, HoursPerWeek=45),\n",
        "Row(EmpID=103, Name=\"Kabir\", Department=\"Marketing\", Project=\"Product Launch\", Salary=65000, HoursPerWeek=40),\n",
        "Row(EmpID=104, Name=\"Anita\", Department=\"Sales\", Project=\"Client Outreach\", Salary=70000, HoursPerWeek=38),\n",
        "Row(EmpID=105, Name=\"Divya\", Department=\"Engineering\", Project=\"AI Engine\", Salary=99000, HoursPerWeek=48),\n",
        "Row(EmpID=106, Name=\"Amit\", Department=\"Marketing\", Project=\"Social Media\",Salary=62000, HoursPerWeek=35),\n",
        "Row(EmpID=107, Name=\"Priya\", Department=\"HR\", Project=\"Policy Revamp\", Salary=58000, HoursPerWeek=37),\n",
        "Row(EmpID=108, Name=\"Manav\", Department=\"Sales\", Project=\"Lead Gen\", Salary=73000, HoursPerWeek=41),\n",
        "Row(EmpID=109, Name=\"Neha\", Department=\"Engineering\", Project=\"Security Suite\",Salary=91000, HoursPerWeek=46),\n",
        "Row(EmpID=110, Name=\"Farah\", Department=\"HR\", Project=\"Onboarding\", Salary=60000, HoursPerWeek=36)\n",
        "]\n",
        "\n",
        "df=spark.createDataFrame(data)\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"employee_local\")\n"
      ],
      "metadata": {
        "id": "QRpFeVGZ_PwP"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceGlobalTempView(\"employee_global\")"
      ],
      "metadata": {
        "id": "iyUn_ROp_klV"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part A: Exercises on Local View (employees_local )\n",
        "spark.sql(\"select * from employee_local where project='AI Engine'\").show()\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "spark.sql(\"select*from employee_local where Department='Marketing'and Salary>60000\").show()\n",
        "\n",
        "print(\"------------------------------------------------------\")\n",
        "spark.sql(\"select Department,avg(Salary)as average_salary from employee_local group by Department\").show()\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "spark.sql(\"select EmpID,name,salary from employee_local order by salary desc limit 3\").show()\n",
        "\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "spark.sql(\"select empid,name,hoursperweek from employee_local where hoursperweek>40\").show()\n",
        "\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "spark.sql(\"select project,count(empid)from employee_local group by project\").show()\n",
        "\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6doIEoAb_rv3",
        "outputId": "a0d57655-ba97-4deb-d16c-975b1a7c709d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------+------+------------+\n",
            "|EmpID| Name| Department|  Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "|  101| Ravi|Engineering|AI Engine| 95000|          42|\n",
            "|  105|Divya|Engineering|AI Engine| 99000|          48|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "\n",
            "------------------------------------------------------\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "|EmpID| Name|Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "|  103|Kabir| Marketing|Product Launch| 65000|          40|\n",
            "|  106| Amit| Marketing|  Social Media| 62000|          35|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "\n",
            "------------------------------------------------------\n",
            "+-----------+--------------+\n",
            "| Department|average_salary|\n",
            "+-----------+--------------+\n",
            "|      Sales|       71500.0|\n",
            "|Engineering|       93000.0|\n",
            "|  Marketing|       63500.0|\n",
            "|         HR|       59000.0|\n",
            "+-----------+--------------+\n",
            "\n",
            "------------------------------------------------------\n",
            "+-----+-----+------+\n",
            "|EmpID| name|salary|\n",
            "+-----+-----+------+\n",
            "|  105|Divya| 99000|\n",
            "|  101| Ravi| 95000|\n",
            "|  109| Neha| 91000|\n",
            "+-----+-----+------+\n",
            "\n",
            "------------------------------------------------------\n",
            "+-----+-----+------------+\n",
            "|empid| name|hoursperweek|\n",
            "+-----+-----+------------+\n",
            "|  101| Ravi|          42|\n",
            "|  102|Sneha|          45|\n",
            "|  105|Divya|          48|\n",
            "|  108|Manav|          41|\n",
            "|  109| Neha|          46|\n",
            "+-----+-----+------------+\n",
            "\n",
            "------------------------------------------------------\n",
            "+---------------+------------+\n",
            "|        project|count(empid)|\n",
            "+---------------+------------+\n",
            "|  Data Platform|           1|\n",
            "|      AI Engine|           2|\n",
            "| Product Launch|           1|\n",
            "|Client Outreach|           1|\n",
            "| Security Suite|           1|\n",
            "|  Policy Revamp|           1|\n",
            "|       Lead Gen|           1|\n",
            "|   Social Media|           1|\n",
            "|     Onboarding|           1|\n",
            "+---------------+------------+\n",
            "\n",
            "------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.dropTempView(\"employee_local\")\n",
        "\n",
        "spark.sql(\"select*from employee_local\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "9eGfe3HAEYvK",
        "outputId": "051eae70-8877-4d89-befc-256ceba57492"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `employee_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 12;\n'Project [*]\n+- 'UnresolvedRelation [employee_local], [], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-781482787.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"employee_local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select*from employee_local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `employee_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 12;\n'Project [*]\n+- 'UnresolvedRelation [employee_local], [], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Part B: Exercises on Global View (employees_global )\n",
        "spark.sql(\"select*from global_temp.employee_global where hoursperweek<38\").show()\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "spark.sql(\"select department,sum(salary)from global_temp.employee_global group by department\").show()\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "spark.sql(\"\"\"select*,\n",
        "              case\n",
        "                 when hoursperweek>45 then 'overworked'\n",
        "                 else'normal'\n",
        "              end as status\n",
        "              from global_temp.employee_global\"\"\").show()\n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "           select project,count(empid)from global_temp.employee_global group by project\"\"\").show()\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "spark.sql(\"\"\" select empid,name,salary as above_average_salary from global_temp.employee_global where salary>(select avg(salary)from global_temp.employee_global)\"\"\").show()\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Devm60tpDAyq",
        "outputId": "f25fdab8-19a7-4dc1-defd-4778f56d8c44"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+-------------+------+------------+\n",
            "|EmpID| Name|Department|      Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "|  106| Amit| Marketing| Social Media| 62000|          35|\n",
            "|  107|Priya|        HR|Policy Revamp| 58000|          37|\n",
            "|  110|Farah|        HR|   Onboarding| 60000|          36|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "\n",
            "---------------------------------------\n",
            "+-----------+-----------+\n",
            "| department|sum(salary)|\n",
            "+-----------+-----------+\n",
            "|      Sales|     143000|\n",
            "|Engineering|     372000|\n",
            "|  Marketing|     127000|\n",
            "|         HR|     118000|\n",
            "+-----------+-----------+\n",
            "\n",
            "---------------------------------------\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|    status|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|    normal|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|    normal|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|    normal|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|    normal|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|overworked|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|    normal|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|    normal|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|    normal|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|overworked|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|    normal|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "\n",
            "---------------------------------------\n",
            "+---------------+------------+\n",
            "|        project|count(empid)|\n",
            "+---------------+------------+\n",
            "|  Data Platform|           1|\n",
            "|      AI Engine|           2|\n",
            "| Product Launch|           1|\n",
            "|Client Outreach|           1|\n",
            "| Security Suite|           1|\n",
            "|  Policy Revamp|           1|\n",
            "|       Lead Gen|           1|\n",
            "|   Social Media|           1|\n",
            "|     Onboarding|           1|\n",
            "+---------------+------------+\n",
            "\n",
            "---------------------------------------\n",
            "+-----+-----+------+\n",
            "|empid| name|salary|\n",
            "+-----+-----+------+\n",
            "|  101| Ravi| 95000|\n",
            "|  102|Sneha| 87000|\n",
            "|  105|Divya| 99000|\n",
            "|  109| Neha| 91000|\n",
            "+-----+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_session=SparkSession.builder.appName(\"new_session\").getOrCreate()\n",
        "new_session.sql(\"select*from global_temp.employee_global\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMC81kSaMEhL",
        "outputId": "7e3d983b-f0be-4bc7-9adb-4f01313ecbf4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bonus challenges\n",
        "spark.sql(\"\"\"select* ,\n",
        "                   rank() over(partition by department\n",
        "                   order by salary desc\n",
        "                   )as depttRank\n",
        "                   from global_temp.employee_global\"\"\").show()\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "  CREATE OR REPLACE TEMP VIEW engineering_only AS\n",
        "  SELECT *\n",
        "  FROM employee_local\n",
        "  WHERE Department = 'Engineering'\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM engineering_only\").show()\n",
        "\n",
        "print(\"--------------------------------\")\n",
        "\n",
        "spark.sql(\"\"\"create or replace temp view active_employees as\n",
        "            select* from global_temp.employee_global where hoursperweek>=38 \"\"\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM active_employees\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIaAE0U6KY_f",
        "outputId": "32d2c58c-7a85-4205-932d-58ae3cab3968"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+---------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|depttRank|\n",
            "+-----+-----+-----------+---------------+------+------------+---------+\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|        1|\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|        2|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|        3|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|        4|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|        1|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|        2|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|        1|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|        2|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|        1|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|        2|\n",
            "+-----+-----+-----------+---------------+------+------------+---------+\n",
            "\n",
            "---------------------------------------\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n",
            "--------------------------------\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}